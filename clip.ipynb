{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad510102",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923096b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qpm5or2f\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-qpm5or2f\n",
      "  Resolved https://github.com/openai/CLIP.git to commit b46f5ac7587d2e1862f8b7b1573179d80dcdd620\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (4.64.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (1.12.0a0+bd13bc6)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (0.13.0a0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->clip==1.0) (4.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->clip==1.0) (9.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.22.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=7b3c00aaaaf9ab2428492f9f8dbc72ca46fe840b1a89f7f53870de3a4768e0d9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y0ua70lx/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install clip-by-openai\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971b7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import json\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "50f02452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58d5f",
   "metadata": {},
   "source": [
    "## Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f81c7e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "87e3e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --------------------------------------------- #\n",
    "#                  Data Utils\n",
    "# --------------------------------------------- #\n",
    "\n",
    "import os, pickle, re, csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import functools\n",
    "\n",
    "image_transforms = transforms.Compose([PIL.Image.fromarray, \n",
    "                                           transforms.Resize((64, 64)),\n",
    "                                           #transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        # dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "        #                       imsize=cfg.IMSIZE,\n",
    "        #                       transform=image_transform)\n",
    "        #assert dataset\n",
    "def video_transform(video, image_transform):\n",
    "    vid = []\n",
    "    for im in video:\n",
    "        vid.append(image_transform(im))\n",
    "    vid = torch.stack(vid).permute(1, 0, 2, 3)\n",
    "    print(\"vid value: \", vid)\n",
    "    return vid\n",
    "\n",
    "video_len = 5\n",
    "n_channels = 3\n",
    "    # functools.partial takes methods/functions as an input\n",
    "video_transforms = functools.partial(video_transform, image_transform=image_transforms)\n",
    "\n",
    "class VideoFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, counter = None, cache=None, min_len=4, mode='train', load_images=False, id_file='train_seen_unseen_ids.npy'):\n",
    "        self.lengths = []\n",
    "        self.followings = []\n",
    "        dataset = ImageFolder(folder)\n",
    "        self.dir_path = folder\n",
    "        self.total_frames = 0\n",
    "        self.images = []\n",
    "        self.labels = np.load(os.path.join(folder, 'labels.npy'), allow_pickle=True, encoding='latin1').item()\n",
    "        if cache is not None and os.path.exists(cache + 'img_cache' + str(min_len) + '.npy') and os.path.exists(cache + 'following_cache' + str(min_len) +  '.npy'):\n",
    "            self.images = np.load(cache + 'img_cache' + str(min_len) + '.npy', encoding='latin1')\n",
    "            self.followings = np.load(cache + 'following_cache' + str(min_len) + '.npy')\n",
    "        else:\n",
    "            for idx, (im, _) in enumerate(\n",
    "                    tqdm(dataset, desc=\"Counting total number of frames\")):\n",
    "                img_path, _ = dataset.imgs[idx]\n",
    "                v_name = img_path.replace(folder,'')\n",
    "                id = v_name.split('/')[-1]\n",
    "                id = int(id.replace('.png', ''))\n",
    "                v_name = re.sub(r\"[0-9]+.png\",'', v_name)\n",
    "                if id > counter[v_name] - min_len:\n",
    "                    continue\n",
    "                following_imgs = []\n",
    "                for i in range(min_len):\n",
    "                    following_imgs.append(v_name + str(id+i+1) + '.png')\n",
    "                self.images.append(img_path.replace(folder, ''))\n",
    "                self.followings.append(following_imgs)\n",
    "            np.save(os.path.join(folder, 'img_cache' + str(min_len) + '.npy'), self.images)\n",
    "            np.save(os.path.join(folder, 'following_cache' + str(min_len) + '.npy'), self.followings)\n",
    "\n",
    "        # train_id, test_id = np.load(self.dir_path + 'train_test_ids.npy', allow_pickle=True, encoding='latin1')\n",
    "        train_id, val_id, test_id = np.load(os.path.join(self.dir_path, id_file), allow_pickle=True)\n",
    "        if mode == 'train':\n",
    "            orders = train_id\n",
    "        elif mode =='val':\n",
    "            orders = val_id[:2320]\n",
    "        elif mode == 'test':\n",
    "            orders = test_id\n",
    "        else:\n",
    "            raise ValueError\n",
    "        orders = np.array(orders).astype('int32')\n",
    "        self.images = self.images[orders]\n",
    "        self.followings = self.followings[orders]\n",
    "        print(\"Total number of clips {}\".format(len(self.images)))\n",
    "\n",
    "        self.image_arrays = {}\n",
    "        if load_images:\n",
    "            for idx, (im, _) in enumerate(\n",
    "                    tqdm(dataset, desc=\"Counting total number of frames\")):\n",
    "                img_path, _ = dataset.imgs[idx]\n",
    "                self.image_arrays[img_path] = im\n",
    "\n",
    "    def sample_image(self, im):\n",
    "        shorter, longer = min(im.size[0], im.size[1]), max(im.size[0], im.size[1])\n",
    "        video_len = int(longer/shorter)\n",
    "        se = np.random.randint(0, video_len, 1)[0]\n",
    "        print(se*shorter, shorter, (se+1)*shorter)\n",
    "        return im.crop((0, se * shorter, shorter, (se+1)*shorter)), se\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        lists = [self.images[item]]\n",
    "        for i in range(len(self.followings[item])):\n",
    "            lists.append(str(self.followings[item][i]))\n",
    "        return lists\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    \n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, textvec, transform):\n",
    "        self.dir_path = dataset.dir_path\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transform\n",
    "        self.descriptions = np.load(textvec + 'descriptions_vec.npy', allow_pickle=True, encoding='latin1').item()\n",
    "        self.attributes =  np.load(textvec + 'descriptions_attr.npy', allow_pickle=True, encoding='latin1').item()\n",
    "        self.subtitles = np.load(textvec + 'subtitles_vec.npy', allow_pickle=True, encoding='latin1').item()\n",
    "        self.descriptions_original = np.load(textvec + 'descriptions.npy', allow_pickle=True, encoding='latin1').item()\n",
    "        self.labels = dataset.labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        path = self.dir_path + str(self.dataset[item][0])[2:-1]\n",
    "        id = str(self.dataset[item][0]).replace('.png','')[2:-1]\n",
    "        img_id = id\n",
    "        im = PIL.Image.open(path)\n",
    "        image, sample_idx = self.dataset.sample_image(im)\n",
    "        image = self.transforms(np.array(image))\n",
    "        subs = self.subtitles[id][0]\n",
    "        se = 0\n",
    "        if len(self.descriptions_original[id]) > 1:\n",
    "            se = np.random.randint(0,len(self.descriptions_original[id]),1)\n",
    "            se = se[0]\n",
    "        des = self.descriptions[id][se]\n",
    "        attri = self.attributes[id][se].astype('float32')\n",
    "        text = self.descriptions_original[id][se]\n",
    "        label = self.labels[id].astype(np.float32)\n",
    "        input_id = None\n",
    "        mask = None\n",
    "        if self.return_caption:\n",
    "            input_id, mask = self._sentence_to_idx(self.tokenized_descriptions[id][se])\n",
    "            input_id = np.array(input_id)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "        lists = self.dataset[item]\n",
    "        content = []\n",
    "        attri_content = []\n",
    "        attri_label = []\n",
    "        for v in lists:\n",
    "            id =str(v).replace('.png','')[2:-1]\n",
    "            se = 0\n",
    "            if len(self.descriptions[id]) > 1:\n",
    "                se = np.random.randint(0,len(self.descriptions[id]),1)\n",
    "                se = se[0]\n",
    "            content.append(np.expand_dims(self.descriptions[id][se], axis = 0))\n",
    "            attri_content.append(np.expand_dims(self.attributes[id][se].astype('float32'), axis = 0))\n",
    "            attri_label.append(np.expand_dims(self.labels[id].astype('float32'), axis = 0))\n",
    "        content = np.concatenate(content, axis = 0)\n",
    "        attri_content = np.concatenate(attri_content, axis = 0)\n",
    "        attri_label = np.concatenate(attri_label, axis = 0)\n",
    "        content = np.concatenate([content, attri_content, attri_label], 1)\n",
    "        des = np.concatenate([des, attri])\n",
    "        ##\n",
    "        content = torch.tensor(content)\n",
    "\n",
    "        data_item = {'images': image, 'text':text, 'description': des,\n",
    "                'subtitle': subs, 'labels':label, 'content': content}\n",
    "        \n",
    "        image_item = image\n",
    "\n",
    "        if self.return_caption:\n",
    "            input_id = torch.tensor(input_id)\n",
    "            mask = torch.tensor(mask)\n",
    "            data_item.update({'input_id': input_id, 'mask':mask})\n",
    "        return image_item\n",
    "        #return data_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f7a89d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import PIL\n",
    "from random import randrange\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "from storygen.config import cfg, cfg_from_file\n",
    "\n",
    "image_transforms = transforms.Compose([PIL.Image.fromarray, \n",
    "                                           transforms.Resize((cfg.IMSIZE, cfg.IMSIZE)),\n",
    "                                           #transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        # dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "        #                       imsize=cfg.IMSIZE,\n",
    "        #                       transform=image_transform)\n",
    "        #assert dataset\n",
    "def video_transform(video, image_transform):\n",
    "    vid = []\n",
    "    for im in video:\n",
    "        vid.append(image_transform(im))\n",
    "    vid = torch.stack(vid).permute(1, 0, 2, 3)\n",
    "    print(\"vid value: \", vid)\n",
    "    return vid\n",
    "\n",
    "video_len = 5\n",
    "n_channels = 3\n",
    "# functools.partial takes methods/functions as an input\n",
    "video_transforms = functools.partial(video_transform, image_transform=image_transforms)\n",
    "    \n",
    "    \n",
    "\n",
    "unique_characters = [\"Wilma\", \"Fred\", \"Betty\", \"Barney\", \"Dino\", \"Pebbles\", \"Mr Slate\"]\n",
    "class VideoFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, cache=None, min_len=1, mode='train'):\n",
    "        self.lengths = []\n",
    "        self.followings = {}\n",
    "        self.dir_path = folder\n",
    "        self.total_frames = 0\n",
    "\n",
    "        # train_id, test_id = np.load(self.dir_path + 'train_test_ids.npy', allow_pickle=True, encoding='latin1')\n",
    "        splits = json.load(open(os.path.join(self.dir_path, 'train-val-test_split.json'), 'r'))\n",
    "        train_id, val_id, test_id = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
    "\n",
    "        if os.path.exists(cache + 'following_cache' + str(min_len) +  '.npy'):\n",
    "            self.followings = pickle.load(open(cache + 'following_cache' + str(min_len) + '.pkl', 'rb'))\n",
    "        else:\n",
    "            all_clips = train_id + val_id + test_id\n",
    "            all_clips.sort()\n",
    "            for idx, clip in enumerate(all_clips):\n",
    "                season, episode = int(clip.split('_')[1]), int(clip.split('_')[3])\n",
    "                has_frames = True\n",
    "                for c in all_clips[idx+1:idx+min_len+1]:\n",
    "                    s_c, e_c = int(c.split('_')[1]), int(c.split('_')[3])\n",
    "                    if s_c != season or e_c != episode:\n",
    "                        has_frames = False\n",
    "                        break\n",
    "                if has_frames:\n",
    "                    self.followings[clip] = all_clips[idx+1:idx+min_len+1]\n",
    "                else:\n",
    "                    continue\n",
    "            pickle.dump(self.followings, open(os.path.join(folder, 'following_cache' + str(min_len) + '.pkl'), 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "        if os.path.exists(os.path.join(folder, 'labels.pkl')):\n",
    "            self.labels = pickle.load(open(os.path.join(folder, 'labels.pkl'), 'rb'))\n",
    "        else:\n",
    "            print(\"Computing and saving labels\")\n",
    "            annotations = json.load(open(os.path.join(folder, 'flintstones_annotations_v1-0.json'), 'r'))\n",
    "            self.labels = {}\n",
    "            for sample in annotations:\n",
    "                sample_characters = [c[\"entityLabel\"].strip().lower() for c in sample[\"characters\"]]\n",
    "                self.labels[sample[\"globalID\"]] = [1 if c.lower() in sample_characters else 0 for c in unique_characters]\n",
    "            pickle.dump(self.labels, open(os.path.join(folder, 'labels.pkl'), 'wb'))\n",
    "\n",
    "        self.embeds = np.load(os.path.join(self.dir_path, \"flintstones_use_embeddings.npy\"))\n",
    "        self.sent2idx = pickle.load(open(os.path.join(self.dir_path, 'flintstones_use_embed_idxs.pkl'), 'rb'))\n",
    "\n",
    "        self.filtered_followings = {}\n",
    "        for i, f in self.followings.items():\n",
    "            #print(f)\n",
    "            if len(f) == 4:\n",
    "                self.filtered_followings[i] = f\n",
    "            else:\n",
    "                continue\n",
    "        self.followings = self.filtered_followings\n",
    "\n",
    "        train_id = [tid for tid in train_id if tid in self.followings]\n",
    "        val_id = [vid for vid in val_id if vid in self.followings]\n",
    "        test_id = [tid for tid in test_id if tid in self.followings]\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.orders = train_id\n",
    "        elif mode =='val':\n",
    "            self.orders = val_id\n",
    "        elif mode == 'test':\n",
    "            self.orders = test_id\n",
    "        else:\n",
    "            raise ValueError\n",
    "        print(\"Total number of clips {}\".format(len(self.orders)))\n",
    "\n",
    "    def sample_image(self, im):\n",
    "        shorter, longer = min(im.size[0], im.size[1]), max(im.size[0], im.size[1])\n",
    "        video_len = int(longer/shorter)\n",
    "        se = np.random.randint(0, video_len, 1)[0]\n",
    "        #print(se*shorter, shorter, (se+1)*shorter)\n",
    "        return im.crop((0, se * shorter, shorter, (se+1)*shorter)), se\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return [self.orders[item]] + self.followings[self.orders[item]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.orders)\n",
    "\n",
    "\n",
    "class StoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform, return_caption=False, out_dir=None, densecap=False):\n",
    "        self.dir_path = dataset.dir_path\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transform\n",
    "        self.labels = dataset.labels\n",
    "        self.return_caption = return_caption\n",
    "\n",
    "        annotations = json.load(open(os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json')))\n",
    "        self.descriptions = {}\n",
    "        for sample in annotations:\n",
    "            self.descriptions[sample[\"globalID\"]] = sample[\"description\"]\n",
    "\n",
    "        if self.return_caption:\n",
    "            self.init_mart_vocab()\n",
    "            self.max_len = self.tokenize_descriptions()\n",
    "            print(\"Max sequence length = %s\" % self.max_len)\n",
    "        else:\n",
    "            self.vocab = None\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "        # if densecap:\n",
    "        #     self.densecap_dataset = DenseCapDataset(self.dir_path)\n",
    "        # else:\n",
    "        self.densecap_dataset = None\n",
    "\n",
    "    def tokenize_descriptions(self):\n",
    "        caption_lengths = []\n",
    "        self.tokenized_descriptions = {}\n",
    "        for img_id, descs in self.descriptions.items():\n",
    "            self.tokenized_descriptions[img_id] = nltk.tokenize.word_tokenize(descs.lower())\n",
    "            caption_lengths.append(len(self.tokenized_descriptions[img_id]))\n",
    "        return max(caption_lengths) + 2\n",
    "\n",
    "    def init_mart_vocab(self):\n",
    "\n",
    "        vocab_file = os.path.join(self.dir_path, 'mart_vocab.pkl')\n",
    "        if os.path.exists(vocab_file):\n",
    "            vocab_from_file = True\n",
    "        else:\n",
    "            vocab_from_file = False\n",
    "\n",
    "        self.vocab = Vocabulary(vocab_threshold=5,\n",
    "                                vocab_file=vocab_file,\n",
    "                                annotations_file=os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json'),\n",
    "                                vocab_from_file=vocab_from_file)\n",
    "\n",
    "    def save_story(self, output, save_path = './'):\n",
    "        all_image = []\n",
    "        images = output['images_numpy']\n",
    "        texts = output['text']\n",
    "        for i in range(images.shape[0]):\n",
    "            all_image.append(np.squeeze(images[i]))\n",
    "        output = PIL.Image.fromarray(np.concatenate(all_image, axis = 0))\n",
    "        output.save(save_path + 'image.png')\n",
    "        fid = open(save_path + 'text.txt', 'w')\n",
    "        for i in range(len(texts)):\n",
    "            fid.write(texts[i] +'\\n' )\n",
    "        fid.close()\n",
    "        return\n",
    "\n",
    "    def _sentence_to_idx(self, sentence_tokens):\n",
    "        \"\"\"[BOS], [WORD1], [WORD2], ..., [WORDN], [EOS], [PAD], ..., [PAD], len == max_t_len\n",
    "        All non-PAD values are valid, with a mask value of 1\n",
    "        \"\"\"\n",
    "        max_t_len = self.max_len\n",
    "        sentence_tokens = sentence_tokens[:max_t_len - 2]\n",
    "\n",
    "        # pad\n",
    "        valid_l = len(sentence_tokens)\n",
    "        mask = [1] * valid_l + [0] * (max_t_len - valid_l)\n",
    "        sentence_tokens += [self.vocab.pad_word] * (max_t_len - valid_l)\n",
    "        input_ids = [self.vocab.word2idx.get(t, self.vocab.word2idx[self.vocab.unk_word]) for t in sentence_tokens]\n",
    "\n",
    "        return input_ids, mask\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        lists = self.dataset[item]\n",
    "        labels = []\n",
    "        images = []\n",
    "        text = []\n",
    "        input_ids = []\n",
    "        masks= []\n",
    "        sent_embeds = []\n",
    "        for idx, globalID in enumerate(lists):\n",
    "            if self.out_dir:\n",
    "                im = PIL.Image.open(os.path.join(self.out_dir, 'img-%s-%s.png' % (item, idx))).convert('RGB')\n",
    "            else:\n",
    "                arr = np.load(os.path.join(self.dir_path, 'video_frames_sampled', globalID + '.npy'))\n",
    "                n_frames = arr.shape[0]\n",
    "                im = arr[randrange(n_frames)]\n",
    "            images.append(np.expand_dims(np.array(im), axis=0))\n",
    "            text.append(self.descriptions[globalID])\n",
    "            labels.append(np.expand_dims(self.labels[globalID], axis = 0))\n",
    "            sent_embeds.append(np.expand_dims(self.dataset.embeds[self.dataset.sent2idx[globalID]], axis = 0))\n",
    "\n",
    "            if self.return_caption:\n",
    "                input_id, mask = self._sentence_to_idx(self.tokenized_descriptions[globalID])\n",
    "                input_ids.append(np.expand_dims(input_id, axis=0))\n",
    "                masks.append(np.expand_dims(mask, axis=0))\n",
    "\n",
    "        sent_embeds = np.concatenate(sent_embeds, axis = 0)\n",
    "        labels = np.concatenate(labels, axis = 0)\n",
    "        images = np.concatenate(images, axis = 0)\n",
    "        # image is T x H x W x C\n",
    "        transformed_images = self.transforms(images)\n",
    "        # After transform, image is C x T x H x W\n",
    "\n",
    "        sent_embeds = torch.tensor(sent_embeds)\n",
    "        labels = torch.tensor(np.array(labels).astype(np.float32))\n",
    "\n",
    "        data_item = {'images': transformed_images, 'text':text, 'description': sent_embeds, 'images_numpy':images, 'labels':labels}\n",
    "\n",
    "        if self.return_caption:\n",
    "            input_ids = torch.tensor(np.concatenate(input_ids))\n",
    "            masks = torch.tensor(np.concatenate(masks))\n",
    "            data_item.update({'input_ids': input_ids, 'masks': masks})\n",
    "\n",
    "        if self.densecap_dataset:\n",
    "            boxes, caps, caps_len = [], [], []\n",
    "            for idx, v in enumerate(lists):\n",
    "                img_id = str(v).replace('.png', '')[2:-1]\n",
    "                path = img_id + '.png'\n",
    "                boxes.append(torch.as_tensor([ann['box'] for ann in self.densecap_dataset[path]], dtype=torch.float32))\n",
    "                caps.append(torch.as_tensor([ann['cap_idx'] for ann in self.densecap_dataset[path]], dtype=torch.long))\n",
    "                caps_len.append(torch.as_tensor([sum([1 for k in ann['cap_idx'] if k!= 0]) for ann in self.densecap_dataset[path]], dtype=torch.long))\n",
    "            targets = {\n",
    "                'boxes': torch.cat(boxes),\n",
    "                'caps': torch.cat(caps),\n",
    "                'caps_len': torch.cat(caps_len),\n",
    "            }\n",
    "            data_item.update(targets)\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.orders)\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform, return_caption=False, densecap=False):\n",
    "        self.dir_path = dataset.dir_path\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transform\n",
    "        self.labels = dataset.labels\n",
    "        self.return_caption = return_caption\n",
    "\n",
    "        annotations = json.load(open(os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json')))\n",
    "        self.descriptions = {}\n",
    "        for sample in annotations:\n",
    "            self.descriptions[sample[\"globalID\"]] = sample[\"description\"]\n",
    "\n",
    "        if self.return_caption:\n",
    "            self.init_mart_vocab()\n",
    "            self.max_len = self.tokenize_descriptions()\n",
    "            print(\"Max sequence length = %s\" % self.max_len)\n",
    "        else:\n",
    "            self.vocab = None\n",
    "\n",
    "        # if densecap:\n",
    "        #     self.densecap_dataset = DenseCapDataset(self.dir_path)\n",
    "        # else:\n",
    "        self.densecap_dataset = None\n",
    "\n",
    "    def tokenize_descriptions(self):\n",
    "        caption_lengths = []\n",
    "        self.tokenized_descriptions = {}\n",
    "        for img_id, descs in self.descriptions.items():\n",
    "            self.tokenized_descriptions[img_id] = nltk.tokenize.word_tokenize(descs.lower())\n",
    "            caption_lengths.append(len(self.tokenized_descriptions[img_id]))\n",
    "        return max(caption_lengths) + 2\n",
    "\n",
    "    def _sentence_to_idx(self, sentence_tokens):\n",
    "        \"\"\"[BOS], [WORD1], [WORD2], ..., [WORDN], [EOS], [PAD], ..., [PAD], len == max_t_len\n",
    "        All non-PAD values are valid, with a mask value of 1\n",
    "        \"\"\"\n",
    "        max_t_len = self.max_len\n",
    "        sentence_tokens = sentence_tokens[:max_t_len - 2]\n",
    "\n",
    "        # pad\n",
    "        valid_l = len(sentence_tokens)\n",
    "        mask = [1] * valid_l + [0] * (max_t_len - valid_l)\n",
    "        sentence_tokens += [self.vocab.pad_word] * (max_t_len - valid_l)\n",
    "        input_ids = [self.vocab.word2idx.get(t, self.vocab.word2idx[self.vocab.unk_word]) for t in sentence_tokens]\n",
    "\n",
    "        return input_ids, mask\n",
    "\n",
    "    def init_mart_vocab(self):\n",
    "\n",
    "        vocab_file = os.path.join(self.dir_path, 'mart_vocab.pkl')\n",
    "        if os.path.exists(vocab_file):\n",
    "            vocab_from_file = True\n",
    "        else:\n",
    "            vocab_from_file = False\n",
    "\n",
    "        self.vocab = Vocabulary(vocab_threshold=5,\n",
    "                                vocab_file=vocab_file,\n",
    "                                annotations_file=os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json'),\n",
    "                                vocab_from_file=vocab_from_file)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # single image input\n",
    "        globalID = self.dataset[item][0]\n",
    "        arr = np.load(os.path.join(self.dir_path, 'video_frames_sampled', globalID + '.npy'))\n",
    "        n_frames = arr.shape[0]\n",
    "        print(\"n_frames: %d\" % n_frames)\n",
    "        im = arr[randrange(n_frames)]\n",
    "        #print(\"img\")\n",
    "        #print(im)\n",
    "        image = np.array(im)\n",
    "        #PIL_image = Image.fromarray(image)\n",
    "        #image = preprocess(Image.open(PIL_image))\n",
    "        image = preprocess(Image.fromarray(image))\n",
    "        print(image.shape)\n",
    "        \n",
    "        text = self.descriptions[globalID]\n",
    "        label = np.array(self.labels[globalID]).astype(np.float32)\n",
    "        sent_embed = self.dataset.embeds[self.dataset.sent2idx[globalID]]\n",
    "\n",
    "        input_id = None\n",
    "        mask = None\n",
    "        if self.return_caption:\n",
    "            input_id, mask = self._sentence_to_idx(self.tokenized_descriptions[globalID])\n",
    "            input_id = np.array(input_id)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "        # input ofr conditional vector\n",
    "        lists = self.dataset[item]\n",
    "        sent_embeds = []\n",
    "        for idx, globalID in enumerate(lists):\n",
    "            sent_embeds.append(np.expand_dims(self.dataset.embeds[self.dataset.sent2idx[globalID]], axis=0))\n",
    "        sent_embeds = np.concatenate(sent_embeds, axis=0)\n",
    "\n",
    "        ##\n",
    "        sent_embeds = torch.tensor(sent_embeds)\n",
    "        #image = self.transforms(image)\n",
    "        data_item = {'images': image, 'text':text, 'description': sent_embed,\n",
    "                     'labels':label, 'content': sent_embeds}\n",
    "\n",
    "        if self.return_caption:\n",
    "            input_id = torch.tensor(input_id)\n",
    "            mask = torch.tensor(mask)\n",
    "            data_item.update({'input_id': input_id, 'mask':mask})\n",
    "\n",
    "        if self.densecap_dataset:\n",
    "            path = globalID + '.png'\n",
    "            try:\n",
    "                _ = self.densecap_dataset[path]\n",
    "            except KeyError:\n",
    "                shorter, longer = min(im.size[0], im.size[1]), max(im.size[0], im.size[1])\n",
    "                video_len = int(longer / shorter)\n",
    "                raise KeyError\n",
    "\n",
    "            boxes = torch.as_tensor([ann['box'] for ann in self.densecap_dataset[path]], dtype=torch.float32)\n",
    "            caps = torch.as_tensor([ann['cap_idx'] for ann in self.densecap_dataset[path]], dtype=torch.long)\n",
    "            caps_len = torch.as_tensor([sum([1 for k in ann['cap_idx'] if k!= 0]) for ann in self.densecap_dataset[path]], dtype=torch.long)\n",
    "            targets = {\n",
    "                'boxes': boxes,\n",
    "                'caps': caps,\n",
    "                'caps_len': caps_len,\n",
    "            }\n",
    "            data_item.update(targets)\n",
    "\n",
    "        #return data_item\n",
    "        return image, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.orders)\n",
    "\n",
    "\n",
    "class StoryImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_folder, im_input_size,\n",
    "                 out_img_folder = None,\n",
    "                 mode='train',\n",
    "                 video_len = 5,\n",
    "                 transform=None):\n",
    "        self.followings = {}\n",
    "        self.data_folder = data_folder\n",
    "        self.labels = pickle.load(open(os.path.join(data_folder, 'labels.pkl'), 'rb'))\n",
    "        self.video_len = video_len\n",
    "        min_len = video_len-1\n",
    "\n",
    "        splits = json.load(open(os.path.join(self.data_folder, 'train-val-test_split.json'), 'r'))\n",
    "        train_ids, val_ids, test_ids = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
    "\n",
    "        if os.path.exists(os.path.join(data_folder, 'following_cache' + str(video_len-1) +  '.pkl')):\n",
    "            self.followings = pickle.load(open(os.path.join(data_folder, 'following_cache' + str(video_len-1) + '.pkl'), 'rb'))\n",
    "        else:\n",
    "            all_clips = train_ids + val_ids + test_ids\n",
    "            all_clips.sort()\n",
    "            for idx, clip in enumerate(tqdm(all_clips, desc=\"Counting total number of frames\")):\n",
    "                season, episode = int(clip.split('_')[1]), int(clip.split('_')[3])\n",
    "                has_frames = True\n",
    "                for c in all_clips[idx+1:idx+min_len+1]:\n",
    "                    s_c, e_c = int(c.split('_')[1]), int(c.split('_')[3])\n",
    "                    if s_c != season or e_c != episode:\n",
    "                        has_frames = False\n",
    "                        break\n",
    "                if has_frames:\n",
    "                    self.followings[clip] = all_clips[idx+1:idx+min_len+1]\n",
    "                else:\n",
    "                    continue\n",
    "            pickle.dump(self.followings, open(os.path.join(self.data_folder, 'following_cache' + str(min_len) + '.pkl'), 'wb'))\n",
    "\n",
    "        self.filtered_followings = {}\n",
    "        for i, f in self.followings.items():\n",
    "            #print(f)\n",
    "            if len(f) == 4:\n",
    "                self.filtered_followings[i] = f\n",
    "            else:\n",
    "                continue\n",
    "        self.followings = self.filtered_followings\n",
    "\n",
    "        train_ids = [tid for tid in train_ids if tid in self.followings]\n",
    "        val_ids = [vid for vid in val_ids if vid in self.followings]\n",
    "        test_ids = [tid for tid in test_ids if tid in self.followings]\n",
    "\n",
    "        # print(list(self.followings.keys())[:10])\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.ids = train_ids\n",
    "            self.transform = transforms.Compose([\n",
    "                # Image.fromarray,\n",
    "                transforms.Resize(im_input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.ids = val_ids[:2060] if mode == \"val\" else test_ids[:2304]\n",
    "            self.transform = transforms.Compose([\n",
    "                # Image.fromarray,\n",
    "                transforms.Resize(im_input_size),\n",
    "                transforms.CenterCrop(im_input_size),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "\n",
    "        self.out_dir = out_img_folder\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        globalIDs = [self.ids[item]] + self.followings[self.ids[item]]\n",
    "\n",
    "        images = []\n",
    "        for idx, globalID in enumerate(globalIDs):\n",
    "            if self.out_dir:\n",
    "                im = PIL.Image.open(os.path.join(self.out_dir, 'img-%s-%s.png' % (item, idx))).convert('RGB')\n",
    "                images.append(im)\n",
    "            else:\n",
    "                arr = np.load(os.path.join(self.data_folder, 'video_frames', globalID + '.npy'))\n",
    "                n_frames = arr.shape[0]\n",
    "                im = arr[randrange(n_frames)]\n",
    "                # images.append(np.expand_dims(np.array(im), axis = 0))\n",
    "                images.append(PIL.Image.fromarray(im))\n",
    "\n",
    "        # print([(type(im)) for im in images])\n",
    "\n",
    "        labels = [self.labels[globalID] for globalID in globalIDs]\n",
    "        return torch.stack([self.transform(image).squeeze(0) for image in images]), torch.tensor(np.vstack(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "243d6171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clips 20132\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dir_path = \"./flintstones_data/\"\n",
    "base = VideoFolderDataset(dir_path, cache = \"./flintstones_data/\", min_len = 4, mode=\"train\")\n",
    "imagedataset = ImageDataset(base, image_transforms)\n",
    "#stimagedataset = StoryImageDataset(base, image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8fd76b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image, text = next(iter(imagedataset))\n",
    "\n",
    "#from PIL import Image\n",
    "#import numpy as np\n",
    "\n",
    "#PIL_image = Image.fromarray(np.uint8(numpy_image)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "96774ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128, 128, 3)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39ee6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#w, h = 512, 512\n",
    "#data = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "#data[0:256, 0:256] = [255, 0, 0] # red patch in upper left\n",
    "#image = image.numpy()\n",
    "#im = Image.fromarray(( image* 255).astype(np.uint8))\n",
    "#Image.fromarray(image).show()\n",
    "#image.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ccd6fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_data():\n",
    "    #dir_path = args.dataset_path\n",
    "    dir_path = \"./pororo_data/\"\n",
    "    counter = np.load(os.path.join(dir_path, 'frames_counter.npy'), allow_pickle=True).item()\n",
    "    print(\"The number of frames: \", len(counter))\n",
    "    base = VideoFolderDataset(dir_path, counter = counter, cache = dir_path, min_len = 4, mode=\"train\")\n",
    "    #storydataset = StoryDataset(base, dir_path, video_transforms)\n",
    "    #imagedataset = ImageDataset(base, dir_path, image_transforms)\n",
    "    storydataset = StoryDataset(base, dir_path, video_transforms)\n",
    "    #train_data = ImagePaths(args.dataset_path, size=256)\n",
    "    #train_loader = DataLoader(imagedataset, batch_size=6, shuffle=False)\n",
    "    story_loader = DataLoader(storydataset, batch_size=6, shuffle=False)\n",
    "    return story_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db39beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list_txt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "\n",
    "# use your own data\n",
    "list_image_path = ['./1.png'] \n",
    "list_txt = ['description for image1.jpg']\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "train_dataloader = DataLoader(dataset) #Define your own dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cabf6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im, ti = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a23272fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77319f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "n_frames: 5\n",
      "torch.Size([3, 224, 224])\n",
      "shapetorch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#if device == \"cpu\":\n",
    "#    model.float()\n",
    "#else :\n",
    "#    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "for epoch in range(10):\n",
    "    for batch in imagedataset :\n",
    "        optimizer.zero_grad()\n",
    "        images, texts = batch\n",
    "        #new_im = np.dstack((images, np.ones(images.shape[:2])))\n",
    "        images =  images.unsqueeze(1)\n",
    "        images = torch.transpose(images, 0, 1)\n",
    "        print(\"shape\" + str(images.shape))\n",
    "        #images = preprocess(images)\n",
    "        texts = clip.tokenize(texts)\n",
    "        \n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        print(images.shape)\n",
    "        print(texts.shape)\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "61a0a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch in train_dataloader :\n",
    "        optimizer.zero_grad()\n",
    "        images,texts = batch\n",
    "        print(len(batch))\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        print(images.shape)\n",
    "        print(texts.shape)\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c52890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0805820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(\"./flintstones_data/video_frames_sampled/s_01_e_01_shot_000099_000173.npy\")\n",
    "#from matplotlib import pyplot as plt\n",
    "#plt.imshow(image, interpolation='nearest')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5907c989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128, 128, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "001894ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 128, 3), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2813\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2813\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 128, 3), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2815\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2815\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2817\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 128, 3), |u1"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from numpy import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
