{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c9b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import json\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39637bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6361 sha256=43f1c23d07465a0f628126b913bc8511786b87697172915f02c2f6b9228b2f85\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-croolvsj/wheels/d3/e0/e9/305e348717e399665119bd012510d51ff4f22d709ff60c3096\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "521fdc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from storygen.config import cfg, cfg_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e8befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Flag enabled:  True\n",
      "number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if cfg.CUDA:\n",
    "    print('CUDA Flag enabled: ', cfg.CUDA)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = './output/%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME)\n",
    "\n",
    "# number of gpus\n",
    "num_gpu = len(cfg.GPU_ID.split(','))\n",
    "print(\"number of GPUs: \", num_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85613c19",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "Basic Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91bd94e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FLAG ENABLED: True\n"
     ]
    }
   ],
   "source": [
    "if cfg.TRAIN.FLAG:\n",
    "    print('TRAIN FLAG ENABLED:', cfg.TRAIN.FLAG)\n",
    "    image_transforms = transforms.Compose([PIL.Image.fromarray, \n",
    "                                           transforms.Resize((cfg.IMSIZE, cfg.IMSIZE)),\n",
    "                                           #transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        # dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "        #                       imsize=cfg.IMSIZE,\n",
    "        #                       transform=image_transform)\n",
    "        #assert dataset\n",
    "    def video_transform(video, image_transform):\n",
    "        vid = []\n",
    "        for im in video:\n",
    "            vid.append(image_transform(im))\n",
    "        vid = torch.stack(vid).permute(1, 0, 2, 3)\n",
    "        print(\"vid value: \", vid)\n",
    "        return vid\n",
    "\n",
    "    video_len = 5\n",
    "    n_channels = 3\n",
    "    # functools.partial takes methods/functions as an input\n",
    "    video_transforms = functools.partial(video_transform, image_transform=image_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2452fb",
   "metadata": {},
   "source": [
    "## GAN, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad249da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import storygen.pororo_data as data\n",
    "from storygen.train import gan_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5809a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of frames:  183\n",
      "Total number of clips 10191\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"./pororo_data/\"\n",
    "counter = np.load(os.path.join(dir_path, 'frames_counter.npy'), allow_pickle=True).item()\n",
    "print(\"The number of frames: \", len(counter))\n",
    "base = data.VideoFolderDataset(dir_path, counter = counter, cache = dir_path, min_len = 4, mode=\"train\")\n",
    "storydataset = data.StoryDataset(base, dir_path, video_transforms)\n",
    "imagedataset = data.ImageDataset(base, dir_path, image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8c8550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imageloader length:  159\n",
      "storyloader length:  159\n",
      "Total number of clips 2320\n",
      "Validation loader length:  116\n"
     ]
    }
   ],
   "source": [
    "# number of gpus\n",
    "num_gpu = len(cfg.GPU_ID.split(','))\n",
    "## dataloader\n",
    "imageloader = torch.utils.data.DataLoader(imagedataset, batch_size=cfg.TRAIN.IM_BATCH_SIZE * num_gpu,\n",
    "                                          drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "print(\"imageloader length: \", len(imageloader))\n",
    "storyloader = torch.utils.data.DataLoader(storydataset, batch_size=cfg.TRAIN.ST_BATCH_SIZE * num_gpu,\n",
    "                                          drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "print(\"storyloader length: \", len(storyloader))\n",
    "\n",
    "## Validation\n",
    "val_dir_path = dir_path\n",
    "base_val = data.VideoFolderDataset(val_dir_path, counter, val_dir_path, 4, mode=\"val\")\n",
    "valdataset = data.StoryDataset(base_val, val_dir_path, video_transforms)\n",
    "valloader = torch.utils.data.DataLoader(valdataset, batch_size=20, \n",
    "                                         drop_last=True, shuffle=False, num_workers=int(cfg.WORKERS))\n",
    "print(\"Validation loader length: \", len(valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b91ca0",
   "metadata": {},
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3807ffcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StoryGAN' from 'model' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m algo \u001b[38;5;241m=\u001b[39m gan_trainer(cfg, output_dir, ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstoryloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTAGE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/storygen/train.py:84\u001b[0m, in \u001b[0;36mgan_trainer.train\u001b[0;34m(self, imageloader, storyloader, testloader, stage)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimagedataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     netG, netD_im, netD_st \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_network_stageI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     netG, netD_im, netD_st \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_network_stageII()\n",
      "File \u001b[0;32m/workspace/storygen/train.py:44\u001b[0m, in \u001b[0;36mgan_trainer.load_network_stageI\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_network_stageI\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StoryGAN, STAGE1_D_IMG, STAGE1_D_STY_V2, StoryMartGAN\n\u001b[1;32m     45\u001b[0m     netG \u001b[38;5;241m=\u001b[39m StoryGAN(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_len)\n\u001b[1;32m     46\u001b[0m     netG\u001b[38;5;241m.\u001b[39mapply(weights_init)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StoryGAN' from 'model' (unknown location)"
     ]
    }
   ],
   "source": [
    "output_dir = './model'\n",
    "algo = gan_trainer(cfg, output_dir, ratio = 1.0)\n",
    "algo.train(imageloader, storyloader, valloader, cfg.STAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ad19441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real_image_batch(imageloader):\n",
    "    imagedataset = None\n",
    "    if imagedataset is None:\n",
    "        imagedataset = enumerate(imageloader)\n",
    "    batch_idx, batch = next(imagedataset)\n",
    "    #print(\"%d %s\" % (batch_idx, batch))\n",
    "    \n",
    "    b = batch\n",
    "    if cfg.CUDA:\n",
    "        for k, v in batch.items():\n",
    "            if k == 'text':\n",
    "                continue\n",
    "            else:\n",
    "                b[k] = v.cuda()\n",
    "    \n",
    "    if batch_idx == len(imageloader) - 1:\n",
    "        imagedataset = enumerate(imageloader)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71649fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'images': tensor([[[[ 0.1608,  0.1451,  0.1294,  ...,  0.3255,  0.5294,  0.4588],\n",
      "          [ 0.1451,  0.1529,  0.2157,  ...,  0.3804,  0.5059,  0.4353],\n",
      "          [ 0.1294,  0.1765,  0.0980,  ...,  0.4824,  0.5216,  0.4196],\n",
      "          ...,\n",
      "          [ 0.8667,  0.7333,  0.6078,  ...,  0.6627,  0.6549,  0.6392],\n",
      "          [ 0.9059,  0.8980,  0.8824,  ...,  0.6471,  0.6471,  0.5765],\n",
      "          [ 0.9059,  0.9059,  0.9059,  ...,  0.6549,  0.6235,  0.3804]],\n",
      "\n",
      "         [[ 0.0902,  0.0824,  0.0824,  ...,  0.2392,  0.4118,  0.3333],\n",
      "          [ 0.0745,  0.0745,  0.1294,  ...,  0.3020,  0.4039,  0.3490],\n",
      "          [ 0.0667,  0.1373,  0.2471,  ...,  0.3882,  0.4196,  0.3490],\n",
      "          ...,\n",
      "          [ 0.7255,  0.5843,  0.4510,  ...,  0.5373,  0.5373,  0.5137],\n",
      "          [ 0.7569,  0.7490,  0.7333,  ...,  0.4980,  0.5137,  0.4431],\n",
      "          [ 0.7569,  0.7569,  0.7569,  ...,  0.5216,  0.4902,  0.2471]],\n",
      "\n",
      "         [[-0.0980, -0.0980, -0.1059,  ..., -0.1922,  0.0353, -0.0588],\n",
      "          [-0.0980, -0.1529, -0.1686,  ..., -0.1608,  0.0039, -0.0745],\n",
      "          [-0.1294, -0.1216,  0.0745,  ..., -0.0745,  0.0118, -0.0745],\n",
      "          ...,\n",
      "          [ 0.3490,  0.2314,  0.1137,  ...,  0.2078,  0.2000,  0.1922],\n",
      "          [ 0.3725,  0.3647,  0.3569,  ...,  0.1765,  0.1922,  0.1216],\n",
      "          [ 0.3725,  0.3725,  0.3725,  ...,  0.1922,  0.1686, -0.0510]]],\n",
      "\n",
      "\n",
      "        [[[-0.6784, -0.6784, -0.7098,  ..., -0.6471, -0.6235, -0.6000],\n",
      "          [-0.2706, -0.3333, -0.5373,  ..., -0.5137, -0.5294, -0.5451],\n",
      "          [ 0.2235,  0.0745, -0.1529,  ..., -0.4824, -0.5137, -0.5451],\n",
      "          ...,\n",
      "          [ 0.6784,  0.6627,  0.6627,  ..., -0.7961, -0.2471,  0.5608],\n",
      "          [ 0.6627,  0.6627,  0.6627,  ..., -0.7804, -0.6549,  0.1608],\n",
      "          [ 0.6706,  0.6627,  0.6627,  ..., -0.6314, -0.7961, -0.3804]],\n",
      "\n",
      "         [[-0.0980, -0.0980, -0.1294,  ..., -0.1608, -0.1451, -0.1294],\n",
      "          [ 0.1843,  0.1451, -0.0039,  ..., -0.0667, -0.0745, -0.0824],\n",
      "          [ 0.5216,  0.4196,  0.2784,  ..., -0.0275, -0.0510, -0.0667],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7725,  0.7725,  ..., -0.4980, -0.0196,  0.6784],\n",
      "          [ 0.7647,  0.7569,  0.7725,  ..., -0.4824, -0.3647,  0.3412],\n",
      "          [ 0.7647,  0.7647,  0.7647,  ..., -0.3569, -0.4902, -0.1294]],\n",
      "\n",
      "         [[ 0.5843,  0.5922,  0.5686,  ...,  0.4980,  0.4980,  0.5137],\n",
      "          [ 0.6863,  0.6784,  0.6235,  ...,  0.5451,  0.5373,  0.5373],\n",
      "          [ 0.8039,  0.7647,  0.7255,  ...,  0.5686,  0.5608,  0.5608],\n",
      "          ...,\n",
      "          [ 0.7961,  0.7882,  0.7882,  ..., -0.0588,  0.2784,  0.7490],\n",
      "          [ 0.7882,  0.7804,  0.7882,  ..., -0.0588,  0.0275,  0.5216],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ...,  0.0353, -0.0588,  0.2000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1137, -0.1843, -0.2078,  ...,  0.1529,  0.3255,  0.0588],\n",
      "          [-0.1059, -0.1608, -0.2078,  ...,  0.0745,  0.2941,  0.0039],\n",
      "          [-0.0980, -0.1451, -0.2000,  ...,  0.0431,  0.3333,  0.1686],\n",
      "          ...,\n",
      "          [-0.0824, -0.0902, -0.0824,  ...,  0.5686,  0.6392,  0.6078],\n",
      "          [-0.0902, -0.0980, -0.0902,  ...,  0.5686,  0.6314,  0.6157],\n",
      "          [-0.0902, -0.0980, -0.0902,  ...,  0.5922,  0.6235,  0.6235]],\n",
      "\n",
      "         [[-0.4588, -0.5137, -0.5373,  ...,  0.0353,  0.5922,  0.4588],\n",
      "          [-0.4510, -0.4980, -0.5373,  ..., -0.0667,  0.5529,  0.4353],\n",
      "          [-0.4431, -0.4745, -0.5216,  ..., -0.1137,  0.5608,  0.5216],\n",
      "          ...,\n",
      "          [-0.4510, -0.4667, -0.4667,  ...,  0.2471,  0.7098,  0.7020],\n",
      "          [-0.4667, -0.4667, -0.4745,  ...,  0.3804,  0.7020,  0.7020],\n",
      "          [-0.4588, -0.4667, -0.4667,  ...,  0.5373,  0.7020,  0.7176]],\n",
      "\n",
      "         [[-0.6471, -0.6784, -0.7020,  ..., -0.0275,  0.8353,  0.9137],\n",
      "          [-0.6392, -0.6627, -0.7020,  ..., -0.0980,  0.8196,  0.9294],\n",
      "          [-0.6392, -0.6549, -0.6863,  ..., -0.1686,  0.7725,  0.8902],\n",
      "          ...,\n",
      "          [-0.6706, -0.6784, -0.6627,  ...,  0.7333,  0.7412,  0.7098],\n",
      "          [-0.6706, -0.6784, -0.6706,  ...,  0.7176,  0.7333,  0.7176],\n",
      "          [-0.6784, -0.6863, -0.6784,  ...,  0.7020,  0.7176,  0.7255]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.1529,  0.2235,  0.2392,  ..., -0.9765, -0.9765, -0.9765],\n",
      "          [ 0.5608,  0.6392,  0.6549,  ..., -0.9922, -0.9922, -0.9843],\n",
      "          [ 0.4275,  0.5373,  0.5451,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [ 0.1765,  0.1843,  0.1451,  ...,  0.6235,  0.6157,  0.6157],\n",
      "          [ 0.0980,  0.0902,  0.0745,  ...,  0.6392,  0.6314,  0.6235],\n",
      "          [-0.2549, -0.2706, -0.2784,  ...,  0.1608,  0.1529,  0.1608]],\n",
      "\n",
      "         [[ 0.2784,  0.3255,  0.3333,  ..., -0.6471, -0.6549, -0.6549],\n",
      "          [ 0.7255,  0.8039,  0.8118,  ..., -0.4824, -0.4824, -0.4902],\n",
      "          [ 0.6157,  0.7098,  0.7098,  ..., -0.4824, -0.4824, -0.4824],\n",
      "          ...,\n",
      "          [ 0.0745,  0.0745,  0.0353,  ...,  0.8118,  0.8196,  0.8196],\n",
      "          [-0.0431, -0.0588, -0.0745,  ...,  0.8353,  0.8275,  0.8275],\n",
      "          [-0.3412, -0.3569, -0.3804,  ...,  0.3020,  0.3020,  0.3098]],\n",
      "\n",
      "         [[ 0.3490,  0.3647,  0.3725,  ..., -0.0588, -0.0745, -0.0824],\n",
      "          [ 0.8275,  0.8902,  0.8980,  ...,  0.4118,  0.4039,  0.4039],\n",
      "          [ 0.7333,  0.8275,  0.8196,  ...,  0.4275,  0.4196,  0.4275],\n",
      "          ...,\n",
      "          [-0.1608, -0.1686, -0.2157,  ...,  0.9451,  0.9451,  0.9451],\n",
      "          [-0.3176, -0.3333, -0.3647,  ...,  0.9608,  0.9529,  0.9529],\n",
      "          [-0.5373, -0.5529, -0.5765,  ...,  0.3961,  0.3882,  0.3882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510, -0.1765, -0.1843,  ..., -0.1529, -0.3647, -0.7490],\n",
      "          [ 0.1373, -0.1294, -0.1137,  ...,  0.1922, -0.0510, -0.6863],\n",
      "          [-0.1451, -0.0431, -0.0118,  ...,  0.3647,  0.1137, -0.6706],\n",
      "          ...,\n",
      "          [ 0.5529,  0.5608,  0.5608,  ...,  0.4980,  0.5059,  0.5059],\n",
      "          [ 0.5373,  0.5451,  0.5451,  ...,  0.5216,  0.5294,  0.5216],\n",
      "          [ 0.5294,  0.5373,  0.5373,  ...,  0.5216,  0.5294,  0.5294]],\n",
      "\n",
      "         [[ 0.4824,  0.4196,  0.4196,  ..., -0.0431, -0.2157, -0.5686],\n",
      "          [ 0.5216,  0.4431,  0.4667,  ...,  0.2941,  0.0588, -0.4902],\n",
      "          [ 0.2157,  0.4980,  0.5294,  ...,  0.4510,  0.2157, -0.4824],\n",
      "          ...,\n",
      "          [ 0.3098,  0.3176,  0.3098,  ...,  0.2235,  0.2235,  0.2314],\n",
      "          [ 0.2941,  0.3020,  0.2941,  ...,  0.2471,  0.2549,  0.2549],\n",
      "          [ 0.2863,  0.2941,  0.2941,  ...,  0.2549,  0.2549,  0.2471]],\n",
      "\n",
      "         [[ 0.8196,  0.9608,  0.9686,  ..., -0.0431, -0.3020, -0.7333],\n",
      "          [ 0.8039,  0.9529,  0.9686,  ...,  0.2941, -0.0118, -0.6784],\n",
      "          [ 0.3961,  0.9294,  0.9686,  ...,  0.4745,  0.1373, -0.6549],\n",
      "          ...,\n",
      "          [-0.0980, -0.0980, -0.0980,  ..., -0.1843, -0.1843, -0.1765],\n",
      "          [-0.1137, -0.1059, -0.1137,  ..., -0.1608, -0.1608, -0.1608],\n",
      "          [-0.1216, -0.1059, -0.1137,  ..., -0.1608, -0.1608, -0.1608]]],\n",
      "\n",
      "\n",
      "        [[[-0.2706, -0.2706, -0.2784,  ..., -0.2392, -0.3490, -0.6784],\n",
      "          [-0.2627, -0.2627, -0.2627,  ..., -0.2549, -0.3412, -0.6784],\n",
      "          [-0.2627, -0.2549, -0.2471,  ..., -0.2549, -0.3725, -0.7098],\n",
      "          ...,\n",
      "          [-0.7098, -0.8039, -0.8118,  ..., -0.7725, -0.7804, -0.7804],\n",
      "          [-0.7490, -0.8510, -0.8902,  ..., -0.7020, -0.6941, -0.7098],\n",
      "          [-0.8902, -0.8824, -0.8824,  ..., -0.4745, -0.3961, -0.4118]],\n",
      "\n",
      "         [[ 0.5137,  0.5137,  0.5059,  ...,  0.5294,  0.3412, -0.3333],\n",
      "          [ 0.5216,  0.5216,  0.5137,  ...,  0.5216,  0.3725, -0.3176],\n",
      "          [ 0.5294,  0.5294,  0.5294,  ...,  0.5294,  0.3020, -0.4039],\n",
      "          ...,\n",
      "          [-0.5294, -0.6706, -0.6706,  ..., -0.5529, -0.5529, -0.5608],\n",
      "          [-0.6000, -0.7255, -0.7569,  ..., -0.4431, -0.4353, -0.4431],\n",
      "          [-0.7725, -0.7725, -0.7569,  ..., -0.1529, -0.0667, -0.0902]],\n",
      "\n",
      "         [[-0.8510, -0.8510, -0.8588,  ..., -0.8196, -0.8431, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8431,  ..., -0.8353, -0.8588, -0.8431],\n",
      "          [-0.8431, -0.8353, -0.8275,  ..., -0.8353, -0.8431, -0.8275],\n",
      "          ...,\n",
      "          [-0.7333, -0.8118, -0.8275,  ..., -0.7647, -0.7725, -0.7804],\n",
      "          [-0.7647, -0.8353, -0.8431,  ..., -0.7333, -0.7412, -0.7490],\n",
      "          [-0.8667, -0.8510, -0.8431,  ..., -0.6314, -0.5922, -0.6000]]]]), 'text': [' rody eddy loopy poby petty and harry are surrounding the table. pororo comes to the table.', ' from far away looking at Loopy pororo asked what Loopy did a minute ago.', \" petty came to rody's side on the playground.\", ' eddy keep singing with his face blushed', ' loopy asks who was that pororo who did mean things.', \" taking pororo's plates loopy is going into the house.\", ' Pororo crong tutu and petty are passing by rody and eddy. friends look upset.', ' pororo crong rody harry poby petty loopy and eddy is sitting at their tables.', ' Pororo gets closer to Loopy and says you were painting.', ' Tongtong takes a motion with his two hands. Tongtong mutters charms.', ' the car is saying looking this side and that.', ' Eddy is very embarrassed. Eddy wonders if the machine is broken.', \" it is snow. in front of the petty's house friends are saying good bye to each other.\", ' after loopy hit the baseball all friends came to loopy and poby give a compliment to loopy', \" crong denies demon's whispering to eat pororo's bread\", ' crong looks at the yellow toy truck and grabs the toy truck with his hands. rody is looking at crong.', ' loopy looks around but nothings happened.', ' Pororo already finished making the dish and covered it.', ' Loopy Harry and Petty are talking about something. There is a bed behind Loopy.', \" harry gets depressed because Harry thinks that Harry is in the same situation as crong's\", ' pororo and eddy are showing off to each other. pororo and eddy are being really competitive.', ' a red car answers to loopy.', ' pororo is angry with crong. pororo has his arms folded.', ' Rody is starting to recover consciousness.', ' Eddy trusts himself and start to run to the beach.', ' On a new toy car Pororo say hi to Loopy. Pororo is going fast.', ' loopy opens her basket and show appetizing breads to her friends', ' rody is trying the salad using fork.', ' Crong is seated on the snow. There is a tin which is full of fish. Crong is looking at the tin.', ' Eddy and Rody are now trying to grasp the frog. However the frog is really good at dodging about the Eddy.', ' poby starts confess a story from a few days ago on his way to go fishing', ' Tongtong is in pain holding tongtong eyes and the red car is happy.', ' Loopy tells Crong that it is nothing.', ' tongtong was told not to worry about the cat doll.', ' Harry says everyone to hold on.', ' It is foggy. The forest is covered with snow. Pororo crong and petty are climbing the foggy mountain.', ' Petty and Loopy go out the house. They are curious about the location friends are.', ' Popo apologizes for his mistake. Pipi tries to find something to revenge.', \" Loopy is smiling. Loopy is saying the reason why loopy is cooking now. Loopy is cooking to prepare tomorrow's picnic. Loopy is wearing a chef hat.\", ' The sky is dark. In the middle of the snow covered forest there is a house.', ' the wind blows hard and the tree bends. pororo and crong are in danger.', ' Loopy holds a red stripe muffler in her two hands.', ' tongtong makes the incantation. suddenly there appeared a big dragon.', ' Rody jumps on the bed. Eddy asks Rody to be careful.', \" A red car is excited. The sky is blue. Pororo's friends are sledding down the snowed hill between snowed trees.\", ' Pororo and Crong are sleeping. Someone puts a gift next to Pororo.', ' looking around the house crong found the toy which pororo was playing with.', ' Pororo and Crong decide to return and ask to Petty.', ' the car is looking up tongtong from over there.', ' loopy is ripping a petal from the flower.', ' Eddy is walking alone pulling eddy sleigh.', ' Crong is surprised to look at the bright stars and a moon hanging from his house ceiling.', ' the can hold one stick and asked tongtong.', ' The sky is orange. A snowman pororo and crong holds a red balloons in their hands. They are floating in the orange sky.', \" eddy went out of the house. pororo is hiding behind eddy's house.\", ' rody asks where Rody is now.', ' pororo found poby eddy crong rody behind the pole. pororo is happy to have found friends.', \" Pororo talks to himself and wonders what Loopy's secret is.\", ' Crong understands what Pororo means to say.', ' Loopy Pororo Crong Rody Poby Harry and the red car starts to ice skate leaving Eddy behind.', ' crong wants to have a hat like that', \" there is a rocket next to eddy's house.\", ' Harry petty Crong Loopy and Poby are wondering why.', ' The sky the land and the forest are all colored green. A snowman raises his hand and says something.'], 'description': tensor([[ 0.3574, -0.0045,  0.0110,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3024, -0.1189,  0.0243,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3320,  0.0132,  0.0030,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.3260, -0.0089,  0.0569,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1472, -0.0905, -0.0612,  ...,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.4689,  0.0298,  0.0899,  ...,  0.0000,  0.0000,  0.0000]]), 'subtitle': tensor([[ 0.0084, -0.0029,  0.0797,  ..., -0.0386,  0.1430,  0.0301],\n",
      "        [ 0.4194, -0.0046,  0.0924,  ..., -0.0545,  0.0223, -0.0029],\n",
      "        [ 0.2713, -0.0229,  0.1329,  ..., -0.0573, -0.0642,  0.0198],\n",
      "        ...,\n",
      "        [ 0.3052, -0.0659,  0.0425,  ..., -0.1239, -0.0336, -0.1344],\n",
      "        [ 0.3886, -0.0101, -0.0139,  ..., -0.0963, -0.0509,  0.0495],\n",
      "        [ 0.3361,  0.0185,  0.0134,  ..., -0.0131, -0.0275, -0.0615]]), 'labels': tensor([[1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 1., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'content': tensor([[[ 0.3574, -0.0045,  0.0110,  ...,  0.0000,  1.0000,  1.0000],\n",
      "         [ 0.3791, -0.0248, -0.0330,  ...,  0.0000,  1.0000,  1.0000],\n",
      "         [ 0.3074,  0.0740,  0.0380,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3277,  0.0775, -0.0036,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2736,  0.0340,  0.0683,  ...,  0.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4897, -0.0690,  0.0220,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5368, -0.1093,  0.0429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4737, -0.0454,  0.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1749, -0.1486,  0.1684,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7155, -0.0551, -0.0310,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3320,  0.0132,  0.0030,  ...,  0.0000,  1.0000,  0.0000],\n",
      "         [ 0.3663,  0.0065, -0.0249,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3167, -0.0199, -0.0455,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2919,  0.0205, -0.0034,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2289,  0.0535, -0.0897,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3260, -0.0089,  0.0569,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4812, -0.1233, -0.0214,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3529, -0.0771,  0.1606,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4308,  0.0771, -0.0195,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3934,  0.0501,  0.0105,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1472, -0.0905, -0.0612,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.2503, -0.0188,  0.0829,  ...,  0.0000,  1.0000,  0.0000],\n",
      "         [ 0.2640, -0.0816, -0.0571,  ...,  0.0000,  1.0000,  0.0000],\n",
      "         [ 0.2460, -0.0175,  0.0400,  ...,  0.0000,  1.0000,  0.0000],\n",
      "         [ 0.2006,  0.0037,  0.0102,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.4689,  0.0298,  0.0899,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3985,  0.0035,  0.0253,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3939, -0.0369,  0.0927,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5028, -0.0890,  0.1285,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6286, -0.0745,  0.0623,  ...,  0.0000,  0.0000,  0.0000]]])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aca42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import PIL\n",
    "from random import randrange\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "unique_characters = [\"Wilma\", \"Fred\", \"Betty\", \"Barney\", \"Dino\", \"Pebbles\", \"Mr Slate\"]\n",
    "class VideoFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, cache=None, min_len=4, mode='train'):\n",
    "        self.lengths = []\n",
    "        self.followings = {}\n",
    "        self.dir_path = folder\n",
    "        self.total_frames = 0\n",
    "\n",
    "        # train_id, test_id = np.load(self.dir_path + 'train_test_ids.npy', allow_pickle=True, encoding='latin1')\n",
    "        splits = json.load(open(os.path.join(self.dir_path, 'train-val-test_split.json'), 'r'))\n",
    "        train_id, val_id, test_id = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
    "\n",
    "        if os.path.exists(cache + 'following_cache' + str(min_len) +  '.npy'):\n",
    "            self.followings = pickle.load(open(cache + 'following_cache' + str(min_len) + '.pkl', 'rb'))\n",
    "        else:\n",
    "            all_clips = train_id + val_id + test_id\n",
    "            all_clips.sort()\n",
    "            for idx, clip in enumerate(all_clips):\n",
    "                season, episode = int(clip.split('_')[1]), int(clip.split('_')[3])\n",
    "                has_frames = True\n",
    "                for c in all_clips[idx+1:idx+min_len+1]:\n",
    "                    s_c, e_c = int(c.split('_')[1]), int(c.split('_')[3])\n",
    "                    if s_c != season or e_c != episode:\n",
    "                        has_frames = False\n",
    "                        break\n",
    "                if has_frames:\n",
    "                    self.followings[clip] = all_clips[idx+1:idx+min_len+1]\n",
    "                else:\n",
    "                    continue\n",
    "            pickle.dump(self.followings, open(os.path.join(folder, 'following_cache' + str(min_len) + '.pkl'), 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "        if os.path.exists(os.path.join(folder, 'labels.pkl')):\n",
    "            self.labels = pickle.load(open(os.path.join(folder, 'labels.pkl'), 'rb'))\n",
    "        else:\n",
    "            print(\"Computing and saving labels\")\n",
    "            annotations = json.load(open(os.path.join(folder, 'flintstones_annotations_v1-0.json'), 'r'))\n",
    "            self.labels = {}\n",
    "            for sample in annotations:\n",
    "                sample_characters = [c[\"entityLabel\"].strip().lower() for c in sample[\"characters\"]]\n",
    "                self.labels[sample[\"globalID\"]] = [1 if c.lower() in sample_characters else 0 for c in unique_characters]\n",
    "            pickle.dump(self.labels, open(os.path.join(folder, 'labels.pkl'), 'wb'))\n",
    "\n",
    "        self.embeds = np.load(os.path.join(self.dir_path, \"flintstones_use_embeddings.npy\"))\n",
    "        self.sent2idx = pickle.load(open(os.path.join(self.dir_path, 'flintstones_use_embed_idxs.pkl'), 'rb'))\n",
    "\n",
    "        self.filtered_followings = {}\n",
    "        for i, f in self.followings.items():\n",
    "            #print(f)\n",
    "            if len(f) == 4:\n",
    "                self.filtered_followings[i] = f\n",
    "            else:\n",
    "                continue\n",
    "        self.followings = self.filtered_followings\n",
    "\n",
    "        train_id = [tid for tid in train_id if tid in self.followings]\n",
    "        val_id = [vid for vid in val_id if vid in self.followings]\n",
    "        test_id = [tid for tid in test_id if tid in self.followings]\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.orders = train_id\n",
    "        elif mode =='val':\n",
    "            self.orders = val_id\n",
    "        elif mode == 'test':\n",
    "            self.orders = test_id\n",
    "        else:\n",
    "            raise ValueError\n",
    "        print(\"Total number of clips {}\".format(len(self.orders)))\n",
    "\n",
    "    def sample_image(self, im):\n",
    "        shorter, longer = min(im.size[0], im.size[1]), max(im.size[0], im.size[1])\n",
    "        video_len = int(longer/shorter)\n",
    "        se = np.random.randint(0, video_len, 1)[0]\n",
    "        #print(se*shorter, shorter, (se+1)*shorter)\n",
    "        return im.crop((0, se * shorter, shorter, (se+1)*shorter)), se\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return [self.orders[item]] + self.followings[self.orders[item]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.orders)\n",
    "\n",
    "\n",
    "class StoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform, return_caption=False, out_dir=None, densecap=False):\n",
    "        self.dir_path = dataset.dir_path\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transform\n",
    "        self.labels = dataset.labels\n",
    "        self.return_caption = return_caption\n",
    "\n",
    "        annotations = json.load(open(os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json')))\n",
    "        self.descriptions = {}\n",
    "        for sample in annotations:\n",
    "            self.descriptions[sample[\"globalID\"]] = sample[\"description\"]\n",
    "\n",
    "        if self.return_caption:\n",
    "            self.init_mart_vocab()\n",
    "            self.max_len = self.tokenize_descriptions()\n",
    "            print(\"Max sequence length = %s\" % self.max_len)\n",
    "        else:\n",
    "            self.vocab = None\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "        # if densecap:\n",
    "        #     self.densecap_dataset = DenseCapDataset(self.dir_path)\n",
    "        # else:\n",
    "        self.densecap_dataset = None\n",
    "\n",
    "    def tokenize_descriptions(self):\n",
    "        caption_lengths = []\n",
    "        self.tokenized_descriptions = {}\n",
    "        for img_id, descs in self.descriptions.items():\n",
    "            self.tokenized_descriptions[img_id] = nltk.tokenize.word_tokenize(descs.lower())\n",
    "            caption_lengths.append(len(self.tokenized_descriptions[img_id]))\n",
    "        return max(caption_lengths) + 2\n",
    "\n",
    "    def init_mart_vocab(self):\n",
    "\n",
    "        vocab_file = os.path.join(self.dir_path, 'mart_vocab.pkl')\n",
    "        if os.path.exists(vocab_file):\n",
    "            vocab_from_file = True\n",
    "        else:\n",
    "            vocab_from_file = False\n",
    "\n",
    "        self.vocab = Vocabulary(vocab_threshold=5,\n",
    "                                vocab_file=vocab_file,\n",
    "                                annotations_file=os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json'),\n",
    "                                vocab_from_file=vocab_from_file)\n",
    "\n",
    "    def save_story(self, output, save_path = './'):\n",
    "        all_image = []\n",
    "        images = output['images_numpy']\n",
    "        texts = output['text']\n",
    "        for i in range(images.shape[0]):\n",
    "            all_image.append(np.squeeze(images[i]))\n",
    "        output = PIL.Image.fromarray(np.concatenate(all_image, axis = 0))\n",
    "        output.save(save_path + 'image.png')\n",
    "        fid = open(save_path + 'text.txt', 'w')\n",
    "        for i in range(len(texts)):\n",
    "            fid.write(texts[i] +'\\n' )\n",
    "        fid.close()\n",
    "        return\n",
    "\n",
    "    def _sentence_to_idx(self, sentence_tokens):\n",
    "        \"\"\"[BOS], [WORD1], [WORD2], ..., [WORDN], [EOS], [PAD], ..., [PAD], len == max_t_len\n",
    "        All non-PAD values are valid, with a mask value of 1\n",
    "        \"\"\"\n",
    "        max_t_len = self.max_len\n",
    "        sentence_tokens = sentence_tokens[:max_t_len - 2]\n",
    "\n",
    "        # pad\n",
    "        valid_l = len(sentence_tokens)\n",
    "        mask = [1] * valid_l + [0] * (max_t_len - valid_l)\n",
    "        sentence_tokens += [self.vocab.pad_word] * (max_t_len - valid_l)\n",
    "        input_ids = [self.vocab.word2idx.get(t, self.vocab.word2idx[self.vocab.unk_word]) for t in sentence_tokens]\n",
    "\n",
    "        return input_ids, mask\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        lists = self.dataset[item]\n",
    "        labels = []\n",
    "        images = []\n",
    "        text = []\n",
    "        input_ids = []\n",
    "        masks= []\n",
    "        sent_embeds = []\n",
    "        for idx, globalID in enumerate(lists):\n",
    "            if self.out_dir:\n",
    "                im = PIL.Image.open(os.path.join(self.out_dir, 'img-%s-%s.png' % (item, idx))).convert('RGB')\n",
    "            else:\n",
    "                arr = np.load(os.path.join(self.dir_path, 'video_frames_sampled', globalID + '.npy'))\n",
    "                n_frames = arr.shape[0]\n",
    "                im = arr[randrange(n_frames)]\n",
    "            images.append(np.expand_dims(np.array(im), axis=0))\n",
    "            text.append(self.descriptions[globalID])\n",
    "            labels.append(np.expand_dims(self.labels[globalID], axis = 0))\n",
    "            sent_embeds.append(np.expand_dims(self.dataset.embeds[self.dataset.sent2idx[globalID]], axis = 0))\n",
    "\n",
    "            if self.return_caption:\n",
    "                input_id, mask = self._sentence_to_idx(self.tokenized_descriptions[globalID])\n",
    "                input_ids.append(np.expand_dims(input_id, axis=0))\n",
    "                masks.append(np.expand_dims(mask, axis=0))\n",
    "\n",
    "        sent_embeds = np.concatenate(sent_embeds, axis = 0)\n",
    "        labels = np.concatenate(labels, axis = 0)\n",
    "        images = np.concatenate(images, axis = 0)\n",
    "        # image is T x H x W x C\n",
    "        transformed_images = self.transforms(images)\n",
    "        # After transform, image is C x T x H x W\n",
    "\n",
    "        sent_embeds = torch.tensor(sent_embeds)\n",
    "        labels = torch.tensor(np.array(labels).astype(np.float32))\n",
    "\n",
    "        data_item = {'images': transformed_images, 'text':text, 'description': sent_embeds, 'images_numpy':images, 'labels':labels}\n",
    "\n",
    "        if self.return_caption:\n",
    "            input_ids = torch.tensor(np.concatenate(input_ids))\n",
    "            masks = torch.tensor(np.concatenate(masks))\n",
    "            data_item.update({'input_ids': input_ids, 'masks': masks})\n",
    "\n",
    "        if self.densecap_dataset:\n",
    "            boxes, caps, caps_len = [], [], []\n",
    "            for idx, v in enumerate(lists):\n",
    "                img_id = str(v).replace('.png', '')[2:-1]\n",
    "                path = img_id + '.png'\n",
    "                boxes.append(torch.as_tensor([ann['box'] for ann in self.densecap_dataset[path]], dtype=torch.float32))\n",
    "                caps.append(torch.as_tensor([ann['cap_idx'] for ann in self.densecap_dataset[path]], dtype=torch.long))\n",
    "                caps_len.append(torch.as_tensor([sum([1 for k in ann['cap_idx'] if k!= 0]) for ann in self.densecap_dataset[path]], dtype=torch.long))\n",
    "            targets = {\n",
    "                'boxes': torch.cat(boxes),\n",
    "                'caps': torch.cat(caps),\n",
    "                'caps_len': torch.cat(caps_len),\n",
    "            }\n",
    "            data_item.update(targets)\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.orders)\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform, return_caption=False, densecap=False):\n",
    "        self.dir_path = dataset.dir_path\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transform\n",
    "        self.labels = dataset.labels\n",
    "        self.return_caption = return_caption\n",
    "\n",
    "        annotations = json.load(open(os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json')))\n",
    "        self.descriptions = {}\n",
    "        for sample in annotations:\n",
    "            self.descriptions[sample[\"globalID\"]] = sample[\"description\"]\n",
    "\n",
    "        if self.return_caption:\n",
    "            self.init_mart_vocab()\n",
    "            self.max_len = self.tokenize_descriptions()\n",
    "            print(\"Max sequence length = %s\" % self.max_len)\n",
    "        else:\n",
    "            self.vocab = None\n",
    "\n",
    "        # if densecap:\n",
    "        #     self.densecap_dataset = DenseCapDataset(self.dir_path)\n",
    "        # else:\n",
    "        self.densecap_dataset = None\n",
    "\n",
    "    def tokenize_descriptions(self):\n",
    "        caption_lengths = []\n",
    "        self.tokenized_descriptions = {}\n",
    "        for img_id, descs in self.descriptions.items():\n",
    "            self.tokenized_descriptions[img_id] = nltk.tokenize.word_tokenize(descs.lower())\n",
    "            caption_lengths.append(len(self.tokenized_descriptions[img_id]))\n",
    "        return max(caption_lengths) + 2\n",
    "\n",
    "    def _sentence_to_idx(self, sentence_tokens):\n",
    "        \"\"\"[BOS], [WORD1], [WORD2], ..., [WORDN], [EOS], [PAD], ..., [PAD], len == max_t_len\n",
    "        All non-PAD values are valid, with a mask value of 1\n",
    "        \"\"\"\n",
    "        max_t_len = self.max_len\n",
    "        sentence_tokens = sentence_tokens[:max_t_len - 2]\n",
    "\n",
    "        # pad\n",
    "        valid_l = len(sentence_tokens)\n",
    "        mask = [1] * valid_l + [0] * (max_t_len - valid_l)\n",
    "        sentence_tokens += [self.vocab.pad_word] * (max_t_len - valid_l)\n",
    "        input_ids = [self.vocab.word2idx.get(t, self.vocab.word2idx[self.vocab.unk_word]) for t in sentence_tokens]\n",
    "\n",
    "        return input_ids, mask\n",
    "\n",
    "    def init_mart_vocab(self):\n",
    "\n",
    "        vocab_file = os.path.join(self.dir_path, 'mart_vocab.pkl')\n",
    "        if os.path.exists(vocab_file):\n",
    "            vocab_from_file = True\n",
    "        else:\n",
    "            vocab_from_file = False\n",
    "\n",
    "        self.vocab = Vocabulary(vocab_threshold=5,\n",
    "                                vocab_file=vocab_file,\n",
    "                                annotations_file=os.path.join(self.dir_path, 'flintstones_annotations_v1-0.json'),\n",
    "                                vocab_from_file=vocab_from_file)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # single image input\n",
    "        globalID = self.dataset[item][0]\n",
    "        arr = np.load(os.path.join(self.dir_path, 'video_frames_sampled', globalID + '.npy'))\n",
    "        n_frames = arr.shape[0]\n",
    "        im = arr[randrange(n_frames)]\n",
    "\n",
    "        image = np.array(im)\n",
    "        text = self.descriptions[globalID]\n",
    "        label = np.array(self.labels[globalID]).astype(np.float32)\n",
    "        sent_embed = self.dataset.embeds[self.dataset.sent2idx[globalID]]\n",
    "\n",
    "        input_id = None\n",
    "        mask = None\n",
    "        if self.return_caption:\n",
    "            input_id, mask = self._sentence_to_idx(self.tokenized_descriptions[globalID])\n",
    "            input_id = np.array(input_id)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "        # input ofr conditional vector\n",
    "        lists = self.dataset[item]\n",
    "        sent_embeds = []\n",
    "        for idx, globalID in enumerate(lists):\n",
    "            sent_embeds.append(np.expand_dims(self.dataset.embeds[self.dataset.sent2idx[globalID]], axis=0))\n",
    "        sent_embeds = np.concatenate(sent_embeds, axis=0)\n",
    "\n",
    "        ##\n",
    "        sent_embeds = torch.tensor(sent_embeds)\n",
    "        image = self.transforms(image)\n",
    "        data_item = {'images': image, 'text':text, 'description': sent_embed,\n",
    "                     'labels':label, 'content': sent_embeds}\n",
    "\n",
    "        if self.return_caption:\n",
    "            input_id = torch.tensor(input_id)\n",
    "            mask = torch.tensor(mask)\n",
    "            data_item.update({'input_id': input_id, 'mask':mask})\n",
    "\n",
    "        if self.densecap_dataset:\n",
    "            path = globalID + '.png'\n",
    "            try:\n",
    "                _ = self.densecap_dataset[path]\n",
    "            except KeyError:\n",
    "                shorter, longer = min(im.size[0], im.size[1]), max(im.size[0], im.size[1])\n",
    "                video_len = int(longer / shorter)\n",
    "                raise KeyError\n",
    "\n",
    "            boxes = torch.as_tensor([ann['box'] for ann in self.densecap_dataset[path]], dtype=torch.float32)\n",
    "            caps = torch.as_tensor([ann['cap_idx'] for ann in self.densecap_dataset[path]], dtype=torch.long)\n",
    "            caps_len = torch.as_tensor([sum([1 for k in ann['cap_idx'] if k!= 0]) for ann in self.densecap_dataset[path]], dtype=torch.long)\n",
    "            targets = {\n",
    "                'boxes': boxes,\n",
    "                'caps': caps,\n",
    "                'caps_len': caps_len,\n",
    "            }\n",
    "            data_item.update(targets)\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.orders)\n",
    "\n",
    "\n",
    "class StoryImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_folder, im_input_size,\n",
    "                 out_img_folder = None,\n",
    "                 mode='train',\n",
    "                 video_len = 5,\n",
    "                 transform=None):\n",
    "        self.followings = {}\n",
    "        self.data_folder = data_folder\n",
    "        self.labels = pickle.load(open(os.path.join(data_folder, 'labels.pkl'), 'rb'))\n",
    "        self.video_len = video_len\n",
    "        min_len = video_len-1\n",
    "\n",
    "        splits = json.load(open(os.path.join(self.data_folder, 'train-val-test_split.json'), 'r'))\n",
    "        train_ids, val_ids, test_ids = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
    "\n",
    "        if os.path.exists(os.path.join(data_folder, 'following_cache' + str(video_len-1) +  '.pkl')):\n",
    "            self.followings = pickle.load(open(os.path.join(data_folder, 'following_cache' + str(video_len-1) + '.pkl'), 'rb'))\n",
    "        else:\n",
    "            all_clips = train_ids + val_ids + test_ids\n",
    "            all_clips.sort()\n",
    "            for idx, clip in enumerate(tqdm(all_clips, desc=\"Counting total number of frames\")):\n",
    "                season, episode = int(clip.split('_')[1]), int(clip.split('_')[3])\n",
    "                has_frames = True\n",
    "                for c in all_clips[idx+1:idx+min_len+1]:\n",
    "                    s_c, e_c = int(c.split('_')[1]), int(c.split('_')[3])\n",
    "                    if s_c != season or e_c != episode:\n",
    "                        has_frames = False\n",
    "                        break\n",
    "                if has_frames:\n",
    "                    self.followings[clip] = all_clips[idx+1:idx+min_len+1]\n",
    "                else:\n",
    "                    continue\n",
    "            pickle.dump(self.followings, open(os.path.join(self.data_folder, 'following_cache' + str(min_len) + '.pkl'), 'wb'))\n",
    "\n",
    "        self.filtered_followings = {}\n",
    "        for i, f in self.followings.items():\n",
    "            #print(f)\n",
    "            if len(f) == 4:\n",
    "                self.filtered_followings[i] = f\n",
    "            else:\n",
    "                continue\n",
    "        self.followings = self.filtered_followings\n",
    "\n",
    "        train_ids = [tid for tid in train_ids if tid in self.followings]\n",
    "        val_ids = [vid for vid in val_ids if vid in self.followings]\n",
    "        test_ids = [tid for tid in test_ids if tid in self.followings]\n",
    "\n",
    "        # print(list(self.followings.keys())[:10])\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.ids = train_ids\n",
    "            self.transform = transforms.Compose([\n",
    "                # Image.fromarray,\n",
    "                transforms.Resize(im_input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.ids = val_ids[:2060] if mode == \"val\" else test_ids[:2304]\n",
    "            self.transform = transforms.Compose([\n",
    "                # Image.fromarray,\n",
    "                transforms.Resize(im_input_size),\n",
    "                transforms.CenterCrop(im_input_size),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "\n",
    "        self.out_dir = out_img_folder\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        globalIDs = [self.ids[item]] + self.followings[self.ids[item]]\n",
    "\n",
    "        images = []\n",
    "        for idx, globalID in enumerate(globalIDs):\n",
    "            if self.out_dir:\n",
    "                im = PIL.Image.open(os.path.join(self.out_dir, 'img-%s-%s.png' % (item, idx))).convert('RGB')\n",
    "                images.append(im)\n",
    "            else:\n",
    "                arr = np.load(os.path.join(self.data_folder, 'video_frames', globalID + '.npy'))\n",
    "                n_frames = arr.shape[0]\n",
    "                im = arr[randrange(n_frames)]\n",
    "                # images.append(np.expand_dims(np.array(im), axis = 0))\n",
    "                images.append(PIL.Image.fromarray(im))\n",
    "\n",
    "        # print([(type(im)) for im in images])\n",
    "\n",
    "        labels = [self.labels[globalID] for globalID in globalIDs]\n",
    "        return torch.stack([self.transform(image).squeeze(0) for image in images]), torch.tensor(np.vstack(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a721b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clips 20132\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dir_path = \"./flintstones_data/\"\n",
    "base = VideoFolderDataset(dir_path, cache = \"./flintstones_data/\", min_len = 4, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31bcd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storydataset = StoryDataset(base, video_transforms)\n",
    "imagedataset = ImageDataset(base, image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3b742da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[-0.4353, -0.4353, -0.4745,  ...,  0.1059, -0.0275,  0.2549],\n",
       "          [-0.4510, -0.4745, -0.4510,  ...,  0.1765,  0.1216,  0.2314],\n",
       "          [-0.4824, -0.5059, -0.4902,  ...,  0.0118,  0.1294,  0.1529],\n",
       "          ...,\n",
       "          [-0.8588, -0.9216, -0.9451,  ...,  0.2078,  0.0745, -0.1451],\n",
       "          [-0.7333, -0.9137, -0.9373,  ...,  0.2314,  0.1216, -0.0275],\n",
       "          [-0.2706, -0.7255, -0.8902,  ...,  0.2078,  0.0667, -0.0039]],\n",
       " \n",
       "         [[-0.4510, -0.4510, -0.5216,  ..., -0.0902, -0.2157,  0.0667],\n",
       "          [-0.4667, -0.4980, -0.5059,  ..., -0.0275, -0.0431,  0.0745],\n",
       "          [-0.4745, -0.5137, -0.5294,  ..., -0.1686, -0.0353,  0.0039],\n",
       "          ...,\n",
       "          [-0.3255, -0.3882, -0.4275,  ...,  0.0588, -0.0431, -0.2314],\n",
       "          [-0.4745, -0.4824, -0.4353,  ...,  0.0902,  0.0196, -0.1137],\n",
       "          [-0.1922, -0.4588, -0.4902,  ...,  0.0667, -0.0275, -0.0980]],\n",
       " \n",
       "         [[-0.6471, -0.6549, -0.7020,  ..., -0.4510, -0.5608, -0.2863],\n",
       "          [-0.6706, -0.6941, -0.6784,  ..., -0.3255, -0.3412, -0.2314],\n",
       "          [-0.6784, -0.6941, -0.6863,  ..., -0.4275, -0.3020, -0.2941],\n",
       "          ...,\n",
       "          [-0.0745, -0.0902, -0.1373,  ..., -0.2941, -0.3490, -0.4667],\n",
       "          [-0.3569, -0.2314, -0.1529,  ..., -0.2627, -0.3098, -0.4039],\n",
       "          [-0.2941, -0.3725, -0.2549,  ..., -0.2706, -0.3569, -0.3961]]]),\n",
       " 'text': 'Fred gets excited and grabs a piece of bread in the dining room. Then Wilma walks in the room.',\n",
       " 'description': array([-1.86015293e-02,  5.10268696e-02,  5.81771098e-02,  2.19927412e-02,\n",
       "        -4.51626303e-03,  5.84185459e-02,  1.53349480e-02,  3.33004184e-02,\n",
       "        -1.78383887e-02, -4.30880859e-02,  4.83047292e-02, -6.69456348e-02,\n",
       "        -8.05703085e-03,  2.70064697e-02, -3.85359526e-02,  9.26304683e-02,\n",
       "         2.03954335e-02, -1.24675725e-02, -3.72440405e-02, -5.03862463e-02,\n",
       "        -6.37345836e-02,  3.80059406e-02,  1.22263413e-02,  6.61345646e-02,\n",
       "        -5.01878113e-02,  2.02991124e-02, -6.48396760e-02, -3.53497192e-02,\n",
       "        -3.29717509e-02, -3.08136716e-02, -2.27735098e-02,  1.13823952e-03,\n",
       "        -3.60811315e-02,  5.29285036e-02,  5.79706766e-02,  1.81615204e-02,\n",
       "         3.54155600e-02, -3.91230024e-02,  6.21154234e-02, -1.57137271e-02,\n",
       "         3.03948615e-02, -1.30035192e-01, -3.32982116e-03, -6.88707829e-02,\n",
       "         6.16490804e-02,  8.28899294e-02, -2.67341491e-02,  3.70583870e-02,\n",
       "        -1.67132728e-02, -1.02046214e-01, -3.17087956e-02, -1.51256761e-02,\n",
       "        -1.25984987e-02,  6.74713477e-02, -4.87682633e-02,  4.26668264e-02,\n",
       "        -2.60353945e-02,  1.63221173e-02,  6.71629161e-02,  1.01628482e-01,\n",
       "        -2.72280760e-02, -6.93611987e-03, -9.35367048e-02,  1.97008774e-02,\n",
       "        -4.12665382e-02, -2.36266572e-02,  8.65586195e-03,  6.08787164e-02,\n",
       "         7.30696023e-02, -1.65748391e-02, -1.41696939e-02,  3.88893299e-02,\n",
       "         3.27722169e-02, -3.46396863e-02,  4.86258045e-03, -1.86182670e-02,\n",
       "        -7.41510093e-02, -8.24690610e-03, -4.43845689e-02,  2.24665869e-02,\n",
       "         5.65123558e-02, -4.29149391e-03,  4.96392362e-02,  2.01346856e-02,\n",
       "        -7.10912272e-02,  9.85184386e-02, -2.50780839e-03, -4.07099389e-02,\n",
       "        -1.70058217e-02, -4.13986742e-02, -3.00231017e-02,  2.68547572e-02,\n",
       "         8.08265507e-02, -2.25274116e-02, -3.27997990e-02, -2.02883743e-02,\n",
       "         4.02571484e-02,  1.68242361e-02,  3.91186289e-02,  1.01970118e-02,\n",
       "         2.04033498e-02,  6.91013038e-02, -8.24454650e-02, -8.95350128e-02,\n",
       "         1.00197643e-03, -3.16427206e-03, -4.61717099e-02,  8.88956711e-03,\n",
       "         2.97401957e-02, -8.49318982e-04, -6.09850138e-02, -2.45173126e-02,\n",
       "        -1.93951000e-02,  6.20722622e-02,  4.83435765e-02, -3.16474214e-02,\n",
       "        -8.37604925e-02,  8.56061727e-02,  1.32623119e-02, -6.58189133e-02,\n",
       "        -3.45377028e-02,  3.02319378e-02, -3.45815788e-03, -1.39551600e-02,\n",
       "        -3.02225128e-02,  3.18541639e-02, -4.64209802e-02, -5.62683083e-02,\n",
       "        -1.88393965e-02,  4.32405388e-03,  1.36880632e-02,  7.48105720e-03,\n",
       "         3.03919334e-02, -2.30105426e-02, -2.64056996e-02,  1.52465631e-03,\n",
       "         9.03172512e-03,  4.14647954e-03,  6.87891338e-03, -3.98284271e-02,\n",
       "        -2.07822118e-02,  2.01783702e-02,  4.45600748e-02, -3.75787704e-03,\n",
       "         1.99808106e-02, -2.31258292e-03, -8.88717622e-02,  1.24407345e-02,\n",
       "        -8.92449077e-03, -4.13024016e-02, -2.32388861e-02,  1.14303678e-02,\n",
       "         1.76263321e-02,  2.70104292e-03,  6.68014064e-02, -5.82354665e-02,\n",
       "        -1.43085932e-02,  2.23094113e-02, -2.23125331e-03, -1.53240934e-02,\n",
       "         3.31115276e-02,  1.54061513e-02,  1.70474779e-02, -3.17861997e-02,\n",
       "        -6.81357784e-03,  4.74876836e-02,  4.49202843e-02, -4.29598615e-02,\n",
       "         4.62848973e-03, -5.42122964e-03,  2.06891671e-02,  1.75614394e-02,\n",
       "        -1.68594476e-02, -4.07764316e-02,  2.08784938e-02, -8.21044967e-02,\n",
       "         5.08988947e-02, -1.14634484e-02,  5.98937552e-03,  4.21963334e-02,\n",
       "        -4.21074741e-02, -1.18309958e-02,  5.92328236e-03, -7.38199651e-02,\n",
       "        -1.09009407e-01, -2.57830359e-02,  2.86288951e-02, -3.21170129e-02,\n",
       "         1.71655100e-02,  1.94574352e-02,  3.78094688e-02, -5.92453703e-02,\n",
       "         5.24503030e-02, -2.31007487e-02, -1.43161081e-02,  5.85940182e-02,\n",
       "        -5.38959280e-02,  2.21854588e-03, -7.13075101e-02,  7.70057179e-03,\n",
       "        -1.48633979e-02, -4.21714932e-02,  3.54210511e-02, -4.11262549e-02,\n",
       "         1.11599285e-02, -2.94623896e-02,  1.40787065e-02,  1.92236900e-02,\n",
       "        -9.08190310e-02,  3.08938324e-03,  2.74608247e-02,  3.87853049e-02,\n",
       "        -3.39984968e-02,  7.01393634e-02,  1.26896128e-02, -8.31780136e-02,\n",
       "         6.84290603e-02,  6.39403565e-03, -6.29771221e-03, -2.38005817e-03,\n",
       "         7.73625821e-02, -7.83107951e-02, -1.14891957e-02,  1.26330480e-02,\n",
       "         4.79249954e-02,  1.53135117e-02,  4.33397815e-02, -9.71467234e-03,\n",
       "        -2.82132011e-02,  6.12031519e-02, -3.10565112e-03, -2.13893596e-02,\n",
       "        -1.10756997e-02, -4.32262868e-02, -2.62421425e-02,  8.73060152e-03,\n",
       "         3.89846899e-02, -6.67520687e-02,  9.11971927e-03,  3.34454700e-02,\n",
       "         6.42815977e-02, -1.01891276e-03, -4.87203859e-02, -2.13361718e-02,\n",
       "        -1.86958537e-02, -4.24139798e-02,  4.22197208e-02, -9.54970659e-04,\n",
       "        -1.07002091e-02, -2.28252895e-02,  7.17162639e-02,  6.85201725e-03,\n",
       "        -6.06676899e-02, -1.87082496e-02,  4.95428219e-02,  6.22932613e-02,\n",
       "        -2.04534009e-02, -1.61338914e-02, -6.93873465e-02,  3.74198593e-02,\n",
       "        -4.25045565e-02, -8.92472193e-02,  2.40823366e-02, -2.29786895e-02,\n",
       "         1.93421764e-03, -2.10011154e-02,  7.52367254e-04, -8.62438828e-02,\n",
       "         2.84701828e-02,  1.16348267e-02, -1.96826886e-02,  4.51300666e-03,\n",
       "        -4.23538461e-02,  3.70705053e-02,  5.83118834e-02,  6.87243743e-03,\n",
       "        -1.18084569e-02,  7.80634880e-02, -1.22002102e-02, -6.64141104e-02,\n",
       "        -1.97846796e-02,  8.09490159e-02, -3.00994096e-03,  6.69644624e-02,\n",
       "         8.27417150e-02, -1.65200455e-03,  6.47083819e-02,  7.06016719e-02,\n",
       "        -2.65549272e-02,  2.70176027e-02,  8.12506303e-02, -3.05788051e-02,\n",
       "        -5.85910976e-02, -3.49800996e-02,  2.38840971e-02, -5.63932545e-02,\n",
       "         4.89367507e-02, -2.28873687e-03, -3.48574631e-02, -1.46002779e-02,\n",
       "        -4.31864299e-02,  6.88124523e-02,  3.06294635e-02,  5.02786674e-02,\n",
       "         9.72002968e-02,  2.80202068e-02, -5.73892444e-02,  2.14749333e-02,\n",
       "        -4.64677066e-02, -2.05677114e-02, -7.67095610e-02, -3.11026704e-02,\n",
       "         3.08633186e-02,  2.88837384e-02,  2.18818318e-02, -4.55078036e-02,\n",
       "         3.10130101e-02, -8.58771242e-03,  3.18516381e-02, -7.85193816e-02,\n",
       "         2.12643351e-02,  7.41171418e-03,  5.09342831e-03, -2.72797551e-02,\n",
       "        -2.27523260e-02, -1.90106072e-02,  5.34515455e-02,  6.90794885e-02,\n",
       "         2.81345341e-02,  6.85407035e-03, -4.07433225e-04, -5.52860461e-02,\n",
       "        -3.54052335e-02,  4.35162559e-02,  5.58123365e-02,  4.07732539e-02,\n",
       "        -6.90862397e-03,  1.92895755e-02, -6.09483197e-02,  2.86079682e-02,\n",
       "         2.51036361e-02,  6.93247244e-02, -1.97983552e-02,  2.09288057e-02,\n",
       "        -4.28187922e-02, -2.54970789e-02,  2.09695892e-03, -2.67138388e-02,\n",
       "        -4.13636602e-02, -3.60949449e-02,  8.52920115e-02,  2.83553060e-02,\n",
       "        -4.60835323e-02,  8.83695334e-02, -1.75240766e-02,  2.97013037e-02,\n",
       "         1.11897127e-04, -5.51012941e-02, -2.43566763e-02, -2.31436826e-02,\n",
       "        -7.74301216e-02,  3.06386817e-02,  2.64395475e-02, -1.80664640e-02,\n",
       "        -4.87514250e-02,  3.29702869e-02, -4.17153202e-02, -5.26640611e-03,\n",
       "        -2.76245754e-02, -3.57195400e-02,  1.27616422e-02,  4.36824970e-02,\n",
       "        -2.99128331e-02,  1.67668127e-02, -8.35034822e-04, -1.79368109e-02,\n",
       "         1.48845220e-03,  1.25976941e-02, -1.18136117e-02,  8.65726694e-02,\n",
       "        -1.64028350e-02, -4.02703024e-02, -6.96577802e-02,  5.35934642e-02,\n",
       "         3.36812460e-04,  2.73617636e-02, -9.46905930e-03,  7.88022280e-02,\n",
       "         4.68476526e-02, -2.42179800e-02, -1.04554534e-01, -2.62010992e-02,\n",
       "         3.60543765e-02, -1.85299048e-03, -2.21201461e-02,  8.93332157e-03,\n",
       "         2.26088408e-02, -2.21827459e-02,  4.65352610e-02, -8.89053494e-02,\n",
       "         3.80135886e-02, -2.05002706e-02, -6.73357472e-02, -3.00776213e-02,\n",
       "        -3.18269916e-02, -6.36440217e-02, -8.87401700e-02,  3.85247171e-02,\n",
       "        -4.67951819e-02, -3.73225585e-02, -2.66030636e-02,  4.45192903e-02,\n",
       "         1.35876790e-01, -5.82126305e-02,  6.16617352e-02,  5.13735674e-02,\n",
       "        -1.61898527e-02,  2.44482327e-02,  3.94305103e-02, -3.99207547e-02,\n",
       "        -2.60481481e-02, -2.05105767e-02,  2.43190471e-02,  6.74215332e-02,\n",
       "         1.87778659e-02,  5.36729433e-02,  5.73340207e-02,  7.49334469e-02,\n",
       "        -1.35903172e-02, -3.71415950e-02,  2.34896839e-02,  4.17965539e-02,\n",
       "         7.13823065e-02, -9.39639471e-03, -5.91544807e-03,  4.81545739e-03,\n",
       "        -8.64236709e-03,  3.92323583e-02, -1.42285198e-01,  7.90676475e-02,\n",
       "         3.25016156e-02,  4.36837599e-02,  7.08106756e-02,  3.19249257e-02,\n",
       "         7.58926617e-03, -3.01080141e-02,  9.85038932e-03,  6.92731515e-02,\n",
       "        -1.63376722e-02,  5.54702207e-02,  2.49603633e-02,  5.31033464e-02,\n",
       "         8.72663334e-02,  2.43974179e-02,  1.76255219e-02, -2.18103509e-02,\n",
       "         5.03369011e-02,  1.33336876e-02,  3.05591413e-04,  4.11147401e-02,\n",
       "         2.50768922e-02,  1.31147038e-02,  1.49757192e-02,  6.44259602e-02,\n",
       "         6.49410719e-03, -6.32506385e-02, -8.59488845e-02,  1.27920704e-02,\n",
       "        -8.72925743e-02, -1.61607694e-02, -2.92494595e-02, -7.13429898e-02,\n",
       "        -1.82110369e-02,  2.98380107e-02, -1.80409178e-02, -1.03239371e-02,\n",
       "        -8.05896372e-02, -9.68139991e-02, -7.25010186e-02, -1.62691996e-02,\n",
       "         2.25277599e-02,  7.98479188e-03,  4.19085799e-03,  1.70090348e-02,\n",
       "         8.31577647e-03,  1.77137833e-02,  3.80539335e-02,  3.61305922e-02,\n",
       "        -6.15149811e-02, -2.16474254e-02, -7.86210299e-02,  5.62068522e-02,\n",
       "        -1.95052624e-02,  4.29616962e-03,  3.58079448e-02,  3.83448265e-02,\n",
       "         1.95962586e-03, -5.98769523e-02,  2.43511312e-02, -2.55520120e-02,\n",
       "        -2.92810611e-02, -3.61710321e-03,  1.92130171e-02,  6.83542490e-02,\n",
       "        -4.83797528e-02,  4.78000380e-02, -1.10048369e-01, -1.94724184e-02,\n",
       "        -6.77937642e-03,  1.61366500e-02, -8.52376074e-02, -3.25117670e-02],\n",
       "       dtype=float32),\n",
       " 'labels': array([0., 1., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'content': tensor([[-0.0186,  0.0510,  0.0582,  ...,  0.0161, -0.0852, -0.0325],\n",
       "         [ 0.0163,  0.0149,  0.0562,  ...,  0.0359, -0.1029,  0.0256],\n",
       "         [ 0.0505,  0.0315,  0.0140,  ..., -0.0251, -0.0919, -0.0916],\n",
       "         [ 0.0242,  0.0579,  0.0153,  ...,  0.0378, -0.0885,  0.0381],\n",
       "         [ 0.0363, -0.0072, -0.0898,  ...,  0.0011, -0.0941,  0.0263]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(imagedataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42865b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
